import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import streamlit as st
import json
import os

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('russian'))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
def load_data():
    path = "/info_data.json"
    if not os.path.exists(path):
        st.error("‚ùå –§–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω! –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ 'data/info_data.json' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    processed_tokens = []
    for token in tokens:
        if token.isalpha() and token not in stop_words:
            lemma = lemmatizer.lemmatize(token)
            processed_tokens.append(lemma)
    return processed_tokens

# –ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
def find_answer(question, knowledge_base):
    processed_question = preprocess_text(question)
    best_match = None
    max_score = 0

    # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for category, subcategories in knowledge_base.items():
        for subcategory, data in subcategories.items():
            keywords = data.get("keywords", [])
            score = sum(1 for keyword in keywords if keyword in processed_question)

            if score > max_score:
                max_score = score
                best_match = data.get("response")

    return best_match if max_score > 0 else "üö´ –ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–æ–π –º–∞—à–∏–Ω–µ."

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
st.set_page_config(page_title="Car Info Bot üöó", page_icon="üöò")

st.title("üöó –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –±–æ—Ç –æ –º–∞—à–∏–Ω–∞—Ö")
st.markdown("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–æ–±–∏–ª—è, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –µ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")

knowledge_base = load_data()

prompt = st.text_input("–ù–∞–ø—Ä–∏–º–µ—Ä: Toyota Camry, BMW X5, Tesla Model 3...")

if prompt:
    response = find_answer(prompt, knowledge_base)
    st.markdown("### üìã –†–µ–∑—É–ª—å—Ç–∞—Ç:")
    st.info(response)


st.markdown("---")
